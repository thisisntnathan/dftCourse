I"R5<p><kbd>SLURM</kbd>, formerly known as the <strong>S</strong>imple <strong>L</strong>inux <strong>U</strong>tility for <strong>R</strong>esource <strong>M</strong>anagement, is a type of program called a workload manager.<sup>1</sup> On large, multi-user systems it can be advantageous and equitable for a program to control the allocation of computational resources, <kbd>SLURM</kbd> does just that.</p>

<p>When you want to run a job on the CHEM cluster you have to ask the <kbd>SLURM</kbd> daemon for resources to allocate to your job. It then takes your script, figures out how much compute power you want, and, if the nodes/memory are available, runs your script on them.  If not, it places them in a queue until the requested resources become available to you.</p>

<!-- ### Commands specific to SLURM -->

<p><kbd>SLURM</kbd> has its own set of commands, and its full documentation can be found <a href="https://slurm.schedmd.com/">here</a>,<sup>2</sup> but here we’ll go over only the most important ones: <code class="language-plaintext highlighter-rouge">sinfo</code>, <code class="language-plaintext highlighter-rouge">pestat</code>, <code class="language-plaintext highlighter-rouge">squeue</code>, <code class="language-plaintext highlighter-rouge">sbatch</code>, and <code class="language-plaintext highlighter-rouge">scancel</code>.</p>

<h3 id="gathering-information">Gathering information</h3>

<p><code class="language-plaintext highlighter-rouge">sinfo</code> gives us information about the status of the cluster’s computing nodes (a node is a single computer in the cluster).</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>nml64@as-chm-cluster | ~ <span class="nv">$ </span>sinfo
PARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST 
chemq        up   infinite      1  alloc chem001 
chemq        up   infinite      5   idle chem[002-006] 
collumq      up   infinite      4  down<span class="k">*</span> dbc[001-003,005] 
collumq      up   infinite      1    mix dbc009 
collumq      up   infinite      1  alloc dbc007 
collumq      up   infinite      4   idle dbc[004,006,008,010] 
widomq       up   infinite      1  drain bw001 
widomq       up   infinite      1    mix bw007 
widomq       up   infinite      5   idle bw[002-006]
</code></pre></div></div>

<p>By default, <code class="language-plaintext highlighter-rouge">sinfo</code> lists the nodes by their partition; the highest level of cluster organization (a <strong>partition</strong> is a set of <strong>compute nodes</strong>). On our shared system nodes are partitioned by ownership, but other systems may have partitions based on usage (e.g. large jobs, small jobs, post-processing, data visualization, etc…) allowing the admin to install different programs on different partitions.</p>

<p>By default <kbd>SLURM</kbd> commands only show us nodes we have access to (more on how to change that below). So, for example, in the above snippet we have 3 partitions <code class="language-plaintext highlighter-rouge">chemq</code>, <code class="language-plaintext highlighter-rouge">collumq</code>, and <code class="language-plaintext highlighter-rouge">widdomq</code>. <code class="language-plaintext highlighter-rouge">collumq</code> has 10 total nodes, 4 of which (dbc1-3 and dbc5) are currently down, 1 of which (dbc7) is fully allocated to a running job, 1 of which (dob9) is mixed, which means it still has resources available, and 4 of which (dbc4,6,8,10) are idle. <code class="language-plaintext highlighter-rouge">sinfo</code> doesn’t provide the most readable output, so sometimes its easier to use <code class="language-plaintext highlighter-rouge">pestat</code>.</p>

<p>Notice how the terminal’s command prompt has changed from <code class="language-plaintext highlighter-rouge">NathanLui@Local</code> to <code class="language-plaintext highlighter-rouge">nml64@as-chm-cluster</code>. This is because I’m now connected to the cluster, instead of working locally on my own computer (more on how to do this in the next section).</p>

<p><code class="language-plaintext highlighter-rouge">pestat</code> is quite similar to <code class="language-plaintext highlighter-rouge">sinfo -N</code> (<code class="language-plaintext highlighter-rouge">-N</code> provides a node-oriented view of the cluster), but I find the layout much easier to read. <code class="language-plaintext highlighter-rouge">pestat</code> also gives us data as to the CPU and memory capacities of each node which will be helpful later.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>nml64@as-chm-cluster | ~ <span class="nv">$ </span>pestat
Hostname       Partition     Node Num_CPU  CPUload  Memsize  Freemem  Joblist
                            State Use/Tot              <span class="o">(</span>MB<span class="o">)</span>     <span class="o">(</span>MB<span class="o">)</span>  JobId User ...
   bw001          widomq    drain<span class="k">*</span>  0  12    0.00     48277    37640   
   bw002          widomq     idle   0  12    0.00     64382    62151   
   bw003          widomq     idle   0  12    0.00     64382    62139   
   bw004          widomq     idle   0  12    0.00     64382    62144   
   bw005          widomq     idle   0  12    0.00     64382    62135   
   bw006          widomq     idle   0  12    0.00     64382    62135   
   bw007          widomq      mix   2  12    1.00<span class="k">*</span>    64382    14752  8609 m----  
 chem001           chemq    alloc  16  16   15.98     31935    24991  8691 j-----  
 chem002           chemq     idle   0  16    0.00     31935    29712   
 chem003           chemq     idle   0  16    0.00     31935    29711   
 chem004           chemq     idle   0  16    0.00     31935    29721   
 chem005           chemq     idle   0  16    0.00     31935    29728   
 chem006           chemq     idle   0  16    0.00     31935    29730   
  dbc001         collumq    down<span class="k">*</span>   0   8    0.00<span class="k">*</span>    16032        0   
  dbc002         collumq    down<span class="k">*</span>   0   8    0.00<span class="k">*</span>     7968        0   
  dbc003         collumq    down<span class="k">*</span>   0   8    0.00<span class="k">*</span>    16032        0   
  dbc004         collumq     idle   0  16    0.00     24085    21791   
  dbc005         collumq    down<span class="k">*</span>   0  16    0.00<span class="k">*</span>    24085        0   
  dbc006         collumq     idle   0  16    0.00     24085    21805   
  dbc007         collumq    alloc  12  12   11.96     32126    26506  8652 nml64  
  dbc008         collumq     idle   0  12    0.00     32126    29959   
  dbc009         collumq      mix  12  40   11.82    192049   182642  8679 nml64  
  dbc010         collumq     idle   0  40    0.00    192049   189551   
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">squeue</code> displays the current job queue:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>nml64@as-chm-cluster | ~ <span class="nv">$ </span>squeue
    JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST 
     8609     chemq matlab_t    m----  R 5-13:38:51      1 bw007 
     8652   collumq trans-Na    nml64  R 3-20:58:56      1 dbc007 
     8679   collumq cis-NaTB    nml64  R 1-03:59:54      1 dbc009 
     8691     chemq A2HMPA3_   j-----  R      39:33      1 chem001 
</code></pre></div></div>

<p>Notice how <code class="language-plaintext highlighter-rouge">squeue</code> gives us a lot of information about the running jobs; it tells us the job number, who’s running the job, the number of node(s), which node(s), their respective partitions, and how long the jobs have been running for.</p>

<p>By default, <code class="language-plaintext highlighter-rouge">squeue</code> and <code class="language-plaintext highlighter-rouge">sinfo</code> only gives us data on the nodes we have permission to use, but if we wanted to check on other nodes we can use the <code class="language-plaintext highlighter-rouge">-all</code> switch.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>nml64@as-chm-cluster | ~ <span class="nv">$ </span>sinfo <span class="nt">-all</span>
Tue Dec 21 15:29:47 2021
PARTITION AVAIL  TIMELIMIT   JOB_SIZE ROOT OVERSUBS     GROUPS  NODES       STATE NODELIST 
chemq        up   infinite 1-infinite   no       NO chemit,col      1   allocated chem001 
chemq        up   infinite 1-infinite   no       NO chemit,col      5        idle chem[002-006] 
slinq        up   infinite 1-infinite   no       NO slin,chemi      1   allocated sl001 
slinq        up   infinite 1-infinite   no       NO slin,chemi      1        idle sl002 
wilsonq      up   infinite 1-infinite   no       NO wilson,che      2        idle jjw[001-002] 
chenq        up   infinite 1-infinite   no       NO chen,chemi      1       mixed pc002 
chenq        up   infinite 1-infinite   no       NO chen,chemi      1        idle pc001 
loringq      up   infinite 1-infinite   no       NO loring,che      2       down<span class="k">*</span> rl[001,003] 
loringq      up   infinite 1-infinite   no       NO loring,che      1     drained rl004 
loringq      up   infinite 1-infinite   no       NO loring,che      1        idle rl002 
collumq      up   infinite 1-infinite   no       NO collum,che      4       down<span class="k">*</span> dbc[001-003,005] 
collumq      up   infinite 1-infinite   no       NO collum,che      1       mixed dbc009 
collumq      up   infinite 1-infinite   no       NO collum,che      1   allocated dbc007 
collumq      up   infinite 1-infinite   no       NO collum,che      4        idle dbc[004,006,008,010] 
widomq       up   infinite 1-infinite   no       NO chemit,col      1     drained bw001 
widomq       up   infinite 1-infinite   no       NO chemit,col      1       mixed bw007 
widomq       up   infinite 1-infinite   no       NO chemit,col      5        idle bw[002-006] 
lambertq     up   infinite 1-infinite   no       NO lambert,ch      2   allocated tl[001-002] 
nml64@as-chm-cluster | ~ <span class="nv">$ </span>squeue <span class="nt">-all</span>
Tue Dec 21 15:30:50 2021
JOBID PARTITION     NAME     USER    STATE       TIME TIME_LIMIT   NODES NODELIST<span class="o">(</span>REASON<span class="o">)</span> 
 8590     slinq B3LYP-D3    y----  RUNNING 6-01:40:46 UNLIMITED        1 sl001 
 8608     chenq matlab_t    m----  RUNNING 5-13:51:21 37-12:00:00      1 pc002 
 8609    widomq matlab_t    m----  RUNNING 5-13:48:40 37-12:00:00      1 bw007 
 8614     chenq matlab_t    m----  RUNNING 5-01:41:36 37-12:00:00      1 pc002 
 8652   collumq trans-Na    nml64  RUNNING 3-21:08:45 17-12:00:00      1 dbc007 
 8679   collumq cis-NaTB    nml64  RUNNING 1-04:09:43 17-12:00:00      1 dbc009 
 8686     slinq B3LYP-D3    y----  RUNNING    3:18:46 UNLIMITED        1 sl001 
 8688     slinq B3LYP-D3    y----  RUNNING      52:00 UNLIMITED        1 sl001 
 8691     chemq A2HMPA3_   j-----  RUNNING      49:22 17-12:00:00      1 chem001 
 8692  lambertq Dimer-6m     k---  RUNNING      19:07 5-00:00:00       1 tl001 
 8693  lambertq Dimer-6m     k---  RUNNING      19:07 5-00:00:00       1 tl002 
</code></pre></div></div>

<p>There are many switches you can use to filter the output of <code class="language-plaintext highlighter-rouge">squeue</code> and <code class="language-plaintext highlighter-rouge">sinfo</code> by user <code class="language-plaintext highlighter-rouge">--user</code>, partition <code class="language-plaintext highlighter-rouge">--partition</code>, node state <code class="language-plaintext highlighter-rouge">--state</code>, etc.</p>

<p>These are some of the most important commands we’ll use in this tutorial. A short cheat sheet can be found <a href="https://slurm.schedmd.com/pdfs/summary.pdf">here</a>.<sup>3</sup></p>

<h3 id="submitting-jobs">Submitting jobs</h3>

<p><code class="language-plaintext highlighter-rouge">sbatch</code> and <code class="language-plaintext highlighter-rouge">scancel</code> are mirror commands.  <code class="language-plaintext highlighter-rouge">sbatch &lt;script&gt;</code> submits the job script to the SLURM daemon for resource allocation, and returns a job ID number. <code class="language-plaintext highlighter-rouge">scancel &lt;job ID&gt;</code> cancels a job after allocation i.e., before or after a job starts running. Any files that have already been written will be preserved as they are when <code class="language-plaintext highlighter-rouge">scancel</code> is executed (keep this in mind if you choose to write any large scratch files to your job directory instead of <code class="language-plaintext highlighter-rouge">/scratch</code>).  In the next section, we’ll learn about how to format submission scripts and submit our first <kbd>SLURM</kbd> job.</p>

<p><br /></p>

<table>
  <tbody>
    <tr>
      <td><center>Previous<br /><a href="/ShortCourse/firstScript.html">My First Script</a></center></td>
      <td><center><a href="/Introduction.html">Home</a></center></td>
      <td><center>Next<br /><a href="/ShortCourse/slurmScripts.html">My First <kbd>SLURM</kbd> Job</a></center></td>
    </tr>
  </tbody>
</table>

<p><br /></p>

<h4 id="references">References</h4>

<p>(1) <a href="https://en.wikipedia.org/wiki/Slurm_Workload_Manager">SLURM Workload Manager</a><br />
(2) <a href="https://slurm.schedmd.com/">SLURM Documentation</a><br />
(3) <a href="https://slurm.schedmd.com/pdfs/summary.pdf">SLURM Cheat Sheet</a></p>
:ET